{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erkamalkumar/Medica_image_segmentation/blob/main/Medical_Images_Segmentation_using_UNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EVifiAYEXWI8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcFzwJcIKeg1"
      },
      "source": [
        "# **Brain MRI Image Segmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezcgP_htGLf_"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle # for accessing kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAupF9FlCbkR"
      },
      "outputs": [],
      "source": [
        "from google.colab import files \n",
        "files.upload() #uploaded our API access of kaggle account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TnTr_B6CmKq"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "BNtFxyJKbYuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNeSUb3bClty"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets list #list of available dataset in kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3222_MIEZ0j"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrFAtQFXEZjD"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d mateuszbuda/lgg-mri-segmentation # for downloading brain MRI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAnW1Mn1Etwy"
      },
      "outputs": [],
      "source": [
        "!unzip lgg-mri-segmentation.zip # for unzipping the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB8ymUT1GLca"
      },
      "outputs": [],
      "source": [
        "import glob # used for matching the specific pattern\n",
        "import time #  used to count the number of seconds elapsed since the epoch\n",
        "import numpy as np # used to perform the mathematical operations on array\n",
        "import pandas as pd # used for analyzing the data\n",
        "from tqdm import tqdm # used for creating Progress Meters or Progress Bars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXcUXUxEGLYx"
      },
      "outputs": [],
      "source": [
        "import cv2 # used for image operations\n",
        "import matplotlib.pyplot as plt # used for visualizing the dataset\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid # Align multiple images of different sizes using ImageGrid\n",
        "from sklearn.model_selection import train_test_split # for splitting our dataset into train and test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6JL_xeJGLVK"
      },
      "outputs": [],
      "source": [
        "import torch # used for providing high-level features using tensors and DL networks\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms as T\n",
        "import albumentations as A # used for image augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6u_6_VUGfyT"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "ROOT_PATH = '/content/lgg-mri-segmentation/kaggle_3m/' # adding the path of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKpbI8r-Gq8B"
      },
      "outputs": [],
      "source": [
        "mask_files = glob.glob(ROOT_PATH + '*/*_mask*') # reading mask images dataset \n",
        "image_files = [file.replace('_mask', '') for file in mask_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg2BekDgGzbT"
      },
      "outputs": [],
      "source": [
        "def diagnosis(mask_path): # function for reading mask images\n",
        "    return 1 if np.max(cv2.imread(mask_path)) > 0 else 0\n",
        "\n",
        "# creating dataframe of mask images\n",
        "df = pd.DataFrame({\"image_path\": image_files,\n",
        "                  \"mask_path\": mask_files,\n",
        "                  \"diagnosis\": [diagnosis(x) for x in mask_files]})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWr6bHc1G7VD"
      },
      "outputs": [],
      "source": [
        "# visualizing dataset with tumer and no tumer\n",
        "ax = df['diagnosis'].value_counts().plot(kind='bar', stacked=True, figsize=(10,6), color=['blue', 'red'])\n",
        "ax.set_title('Data Distribution', fontsize=15)\n",
        "ax.set_ylabel('Total Images', fontsize=15)\n",
        "ax.set_xticklabels(['No Tumor', 'Tumor'], fontsize=12, rotation=0)\n",
        "for i, rows in enumerate(df['diagnosis'].value_counts().values):\n",
        "    ax.annotate(int(rows), xy=(i, rows+12), ha='center', fontweight='bold', fontsize=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxUt2V-8HSqS"
      },
      "outputs": [],
      "source": [
        "# splitting dataset into train and test dataset\n",
        "train_df, val_df = train_test_split(df, stratify=df['diagnosis'], test_size=0.1)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "train_df, test_df = train_test_split(train_df, stratify=train_df['diagnosis'], test_size=0.15)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "print(\"Train: {}\\nVal: {}\\nTest: {}\".format(train_df.shape, val_df.shape, test_df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlyMetlwM9Q_"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 521 # defining image size\n",
        "images = [] # creating a list for storing the MRI images\n",
        "masks = [] # creating a list for storing the mask images\n",
        "df_positive = df[df['diagnosis']==1].sample(5).values\n",
        "\n",
        "# loop for filtering and storing orimginal image and mask images \n",
        "for data in df_positive:\n",
        "    img = cv2.resize(cv2.imread(data[0]), (IMG_SIZE, IMG_SIZE))\n",
        "    mask = cv2.resize(cv2.imread(data[1]), (IMG_SIZE, IMG_SIZE))\n",
        "    images.append(img)\n",
        "    masks.append(mask)\n",
        "images = np.hstack(np.array(images))\n",
        "masks = np.hstack(np.array(masks))\n",
        "\n",
        "fig = plt.figure(figsize=(25,25)) # defining figure size\n",
        "grid = ImageGrid(fig, 111, nrows_ncols=(3,1), axes_pad=0.5)\n",
        "\n",
        "# showing images and mask images\n",
        "grid[0].imshow(images)\n",
        "grid[0].set_title('Images', fontsize=15)\n",
        "grid[0].axis('off')\n",
        "grid[1].imshow(masks)\n",
        "grid[1].set_title('Masks', fontsize=15)\n",
        "grid[1].axis('off')\n",
        "grid[2].imshow(images)\n",
        "grid[2].imshow(masks, alpha=0.4)\n",
        "grid[2].set_title('Brain MRI with mask', fontsize=15)\n",
        "grid[2].axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN9vHi6ONHQf"
      },
      "outputs": [],
      "source": [
        "# function for data augumentation\n",
        "def show_aug(inputs, nrows=5, ncols=5, image=True):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.subplots_adjust(wspace=0., hspace=0.)\n",
        "    i = 0\n",
        "    if len(inputs) > 25:\n",
        "        inputs = inputs[:25]\n",
        "        \n",
        "    for idx in range(len(inputs)):\n",
        "        if image is True:           \n",
        "            img = inputs[idx].numpy().transpose(1,2,0)\n",
        "            mean = [0.485, 0.456, 0.406]\n",
        "            std = [0.229, 0.224, 0.225] \n",
        "            img = (img * std + mean).astype(np.float32)\n",
        "        else:\n",
        "            img = inputs[idx].numpy().astype(np.float32)\n",
        "            img = img[0,:,:]\n",
        "        plt.subplot(nrows, ncols, i+1)\n",
        "        plt.imshow(img); \n",
        "        plt.axis('off')\n",
        "        i += 1\n",
        "    return plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSOsZYG4NUkv"
      },
      "outputs": [],
      "source": [
        "# This function converts NumPy arrays and Python numerical values into PyTorch Tensors\n",
        "class BrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.imread(self.df.iloc[idx, 0])\n",
        "        image = np.array(image)/255.\n",
        "        mask = cv2.imread(self.df.iloc[idx, 1], 0)\n",
        "        mask = np.array(mask)/255.\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            aug = self.transform(image=image, mask=mask)\n",
        "            image = aug['image']\n",
        "            mask = aug['mask']\n",
        "            \n",
        "        image = image.transpose((2,0,1))\n",
        "        image = torch.from_numpy(image).type(torch.float32)\n",
        "        image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image)\n",
        "        mask = np.expand_dims(mask, axis=-1).transpose((2,0,1))\n",
        "        mask = torch.from_numpy(mask).type(torch.float32)\n",
        "        \n",
        "        return image, mask\n",
        "    \n",
        "train_transform = A.Compose([\n",
        "    A.Resize(width=128, height=128, p=1.0),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n",
        "])\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(width=128, height=128, p=1.0),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "])\n",
        "test_transform = A.Compose([\n",
        "    A.Resize(width=128, height=128, p=1.0)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYL4ucmNNc4_"
      },
      "outputs": [],
      "source": [
        "# assigning train dataset and dataloader\n",
        "train_dataset = BrainDataset(train_df, train_transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=26, shuffle=True, num_workers=2)\n",
        "\n",
        "# Defining validation dataset and dataloader\n",
        "val_dataset = BrainDataset(val_df, val_transform)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=26, shuffle=True, num_workers=2)\n",
        "\n",
        "# defining test dataset and dataloader\n",
        "test_dataset = BrainDataset(test_df, test_transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=26, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63OdsPR7Nh3_"
      },
      "outputs": [],
      "source": [
        "images, masks = next(iter(train_dataloader))\n",
        "print(images.shape) #printing image shape\n",
        "print(masks.shape) #prining mask images\n",
        "show_aug(images) # showing augumentation images\n",
        "show_aug(masks, image=False) # showing mask images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzvN-rpKNmlv"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module): # Applies a 2D convolution over an input planes\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\" # repeat 2 times in each iteration\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "            # this is used for adding the modules sequentially\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True))\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x) # returning double convolotuion\n",
        "\n",
        " # Downscaling with maxpool then double conv   \n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels))\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "    \n",
        "# Upscaling then double conv\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear: #if bilinear, use the normal convolutions to reduce the number of channels\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels//2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        \n",
        "        x1 = F.pad(x1, [diffX//2, diffX-diffX//2,\n",
        "                        diffY//2, diffY-diffY//2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "# class for out-convolution by using sequential model\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.Sigmoid())\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5cs6usZN1q_"
      },
      "outputs": [],
      "source": [
        "# unet model implementation\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels # total number of channels used\n",
        "        self.n_classes = n_classes # number of classes\n",
        "        self.bilinear = bilinear \n",
        "        \n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128) # usess down convolution and doing double \n",
        "        self.down2 = Down(128, 256) # usess down convolution and doing double \n",
        "        self.down3 = Down(256, 512) # usess down convolution and doing double \n",
        "        factor = 2 if bilinear else 1 # defining factor \n",
        "        self.down4 = Down(512, 1024//factor) \n",
        "        self.up1 = Up(1024, 512//factor, bilinear) # uses up convolution by using factor and bilinear\n",
        "        self.up2 = Up(512, 256//factor, bilinear)  # uses up convolution by using factor and bilinear      \n",
        "        self.up3 = Up(256, 128//factor, bilinear)  # uses up convolution by using factor and bilinear      \n",
        "        self.up4 = Up(128, 64, bilinear) # uses up convolution by using bilinear       \n",
        "        self.outc = OutConv(64, n_classes) # final out convolution\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2UhdzkmN7Wg"
      },
      "outputs": [],
      "source": [
        "model = UNet(3, 1).to(device)\n",
        "out = model(torch.randn(1, 3, 256, 256).to(device))\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI9Sb4_cOA5f"
      },
      "outputs": [],
      "source": [
        "# function for calculating dice coefficient metric\n",
        "def dice_coef_metric(pred, label):\n",
        "    intersection = 2.0 * (pred * label).sum()\n",
        "    union = pred.sum() + label.sum()\n",
        "    if pred.sum() == 0 and label.sum() == 0:\n",
        "        return 1.\n",
        "    return intersection / union\n",
        "\n",
        "# function for loss prediction\n",
        "def dice_coef_loss(pred, label):\n",
        "    smooth = 1.0\n",
        "    intersection = 2.0 * (pred * label).sum() + smooth\n",
        "    union = pred.sum() + label.sum() + smooth\n",
        "    return 1 - (intersection / union)\n",
        "def bce_dice_loss(pred, label):\n",
        "    dice_loss = dice_coef_loss(pred, label)\n",
        "    bce_loss = nn.BCELoss()(pred, label)\n",
        "    return dice_loss + bce_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa1DUpTcOGof"
      },
      "outputs": [],
      "source": [
        "def compute_iou(model, loader, threshold=0.3):\n",
        "    valloss = 0\n",
        "    with torch.no_grad():\n",
        "        for step, (data, target) in enumerate(loader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            outputs = model(data)\n",
        "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
        "            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n",
        "            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n",
        "\n",
        "            loss = dice_coef_metric(out_cut, target.data.cpu().numpy())\n",
        "            valloss += loss\n",
        "\n",
        "    return valloss / step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUD1RArDOLgw"
      },
      "outputs": [],
      "source": [
        "def train_model(train_loader, val_loader, loss_func, optimizer, scheduler, num_epochs):\n",
        "    loss_history = []\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        \n",
        "        losses = []\n",
        "        train_iou = []\n",
        "        \n",
        "        for i, (image, mask) in enumerate(tqdm(train_loader)):\n",
        "            image = image.to(device)\n",
        "            mask = mask.to(device)\n",
        "            outputs = model(image)\n",
        "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
        "            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
        "            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0            \n",
        "            \n",
        "            train_dice = dice_coef_metric(out_cut, mask.data.cpu().numpy())\n",
        "            loss = loss_func(outputs, mask)\n",
        "            losses.append(loss.item())\n",
        "            train_iou.append(train_dice)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "                \n",
        "        val_mean_iou = compute_iou(model, val_loader)\n",
        "        scheduler.step(val_mean_iou)\n",
        "        loss_history.append(np.array(losses).mean())\n",
        "        train_history.append(np.array(train_iou).mean())\n",
        "        val_history.append(val_mean_iou)\n",
        "        \n",
        "        print('Epoch : {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('loss: {:.3f} - dice_coef: {:.3f} - val_dice_coef: {:.3f}'.format(np.array(losses).mean(),\n",
        "                                                                               np.array(train_iou).mean(),\n",
        "                                                                               val_mean_iou))\n",
        "    return loss_history, train_history, val_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGj_fvtYOT1K"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
        "num_epochs = 100\n",
        "loss_history, train_history, val_history = train_model(train_dataloader, val_dataloader, bce_dice_loss, optimizer, scheduler, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN8yB2UCOdQh"
      },
      "outputs": [],
      "source": [
        "# function for visualizing train and validation history\n",
        "def plot_model_history(model_name, train_history, val_history, num_epochs):\n",
        "    \n",
        "    x = np.arange(num_epochs)\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, train_history, label='train dice', lw=3, c=\"b\")\n",
        "    plt.plot(x, val_history, label='validation dice', lw=3, c=\"r\")\n",
        "\n",
        "    plt.title(f\"{model_name}\", fontsize=15)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.xlabel(\"Epoch\", fontsize=15)\n",
        "    plt.ylabel(\"Dice\", fontsize=15)\n",
        "\n",
        "    plt.show() # used for showing the plot\n",
        "    \n",
        "plot_model_history('UNet', train_history, val_history, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AZDao5gOrMx"
      },
      "outputs": [],
      "source": [
        "test_iou = compute_iou(model, test_dataloader) # for testing IoU\n",
        "print(\"Mean IoU: {:.3f}%\".format(100*test_iou))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6xVCpVuOsQn"
      },
      "outputs": [],
      "source": [
        "test_sample = test_df[test_df[\"diagnosis\"] == 1].sample(1).values[0]\n",
        "image = cv2.resize(cv2.imread(test_sample[0]), (128, 128))\n",
        "mask = cv2.resize(cv2.imread(test_sample[1]), (128, 128))\n",
        "\n",
        "# prediction\n",
        "pred = torch.tensor(image.astype(np.float32) / 255.).unsqueeze(0).permute(0,3,1,2)\n",
        "pred = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(pred)\n",
        "pred = model(pred.to(device))\n",
        "pred = pred.detach().cpu().numpy()[0,0,:,:]\n",
        "\n",
        "pred_t = np.copy(pred)\n",
        "pred_t[np.nonzero(pred_t < 0.3)] = 0.0\n",
        "pred_t[np.nonzero(pred_t >= 0.3)] = 255.\n",
        "pred_t = pred_t.astype(\"uint8\")\n",
        "\n",
        "# plotting the data with image, mask and prediction of image amd mask\n",
        "fig, ax = plt.subplots(nrows=2,  ncols=2, figsize=(10, 10))\n",
        "\n",
        "ax[0, 0].imshow(image)\n",
        "ax[0, 0].set_title(\"image\")\n",
        "ax[0, 1].imshow(mask)\n",
        "ax[0, 1].set_title(\"mask\")\n",
        "ax[1, 0].imshow(pred)\n",
        "ax[1, 0].set_title(\"prediction\")\n",
        "ax[1, 1].imshow(pred_t)\n",
        "ax[1, 1].set_title(\"prediction with threshold\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = test_df[test_df[\"diagnosis\"] == 1].sample(105).values\n",
        "\n",
        "def batch_preds_overlap(model, samples):\n",
        "    \"\"\"\n",
        "    Computes prediction on the dataset\n",
        "    \n",
        "    Returns: list with images overlapping with predictions\n",
        "    \n",
        "    \"\"\"\n",
        "    prediction_overlap = []\n",
        "    for test_sample in samples:\n",
        "\n",
        "         # sample\n",
        "        image = cv2.resize(cv2.imread(test_sample[0]),(128, 128))\n",
        "        image =  image / 255.\n",
        "        ground_truth = cv2.resize(cv2.imread(test_sample[1], 0), (128, 128)).astype(\"uint8\")\n",
        "\n",
        "        # prediction\n",
        "        prediction = torch.tensor(image).unsqueeze(0).permute(0,3,1,2)\n",
        "        prediction = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(prediction)\n",
        "        prediction = model(prediction.to(device).float())\n",
        "        prediction = prediction.detach().cpu().numpy()[0,0,:,:]\n",
        "\n",
        "        prediction[np.nonzero(prediction < 0.3)] = 0.0\n",
        "        prediction[np.nonzero(prediction >= 0.3)] = 255.\n",
        "        prediction = prediction.astype(\"uint8\")\n",
        "\n",
        "        # overlap \n",
        "        original_img = cv2.resize(cv2.imread(test_sample[0]),(128, 128))\n",
        "\n",
        "        _, thresh_gt = cv2.threshold(ground_truth, 127, 255, 0)\n",
        "        _, thresh_p = cv2.threshold(prediction, 127, 255, 0)\n",
        "        contours_gt, _ = cv2.findContours(thresh_gt, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        contours_p, _ = cv2.findContours(thresh_p, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        overlap_img = cv2.drawContours(original_img, contours_gt, 0, (0, 255, 0), 1)\n",
        "        overlap_img = cv2.drawContours(overlap_img, contours_p, 0, (255,36,0), 1)#255,0,0\n",
        "        prediction_overlap.append(overlap_img)\n",
        "\n",
        "    return prediction_overlap\n",
        "    \n",
        "prediction_overlap_r = batch_preds_overlap(model, test_samples)"
      ],
      "metadata": {
        "id": "rQFdtqziS0l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_overlap_5x1_r = []\n",
        "pred_overlap_5x3_r = []\n",
        "\n",
        "for i in range(5, 105+5, 5):\n",
        "    pred_overlap_5x1_r.append(np.hstack(np.array(prediction_overlap_r[i-5:i])))\n",
        "\n",
        "for i in range(3, 21+3, 3):\n",
        "    pred_overlap_5x3_r.append(np.vstack(pred_overlap_5x1_r[i-3:i]))"
      ],
      "metadata": {
        "id": "GCUSnkvZKfD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_plate_overlap(batch_preds, title, num):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.imshow(batch_preds)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.figtext(0.76,0.75,\"Green - Ground Truth\", va=\"center\", ha=\"center\", size=20,color=\"lime\");\n",
        "    plt.figtext(0.26,0.75,\"Red - Prediction\", va=\"center\", ha=\"center\", size=20, color=\"#ff0d00\");\n",
        "    plt.suptitle(title, y=.80, fontsize=20, weight=\"bold\", color=\"#00FFDE\");\n",
        "\n",
        "    fn = \"_\".join((title+str(num)).lower().split()) + \".png\"\n",
        "    plt.savefig(fn, bbox_inches='tight', pad_inches=0.2, transparent=False, facecolor='black')\n",
        "    plt.close()\n",
        "\n",
        "title = \"Predictions of UNet\"\n",
        "\n",
        "for num, batch in enumerate(pred_overlap_5x3_r):\n",
        "    plot_plate_overlap(batch,title, num)"
      ],
      "metadata": {
        "id": "tXHyhSB6KgHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def make_gif(title):\n",
        "    base_name = \"_\".join(title.lower().split())\n",
        "\n",
        "    base_len = len(base_name) \n",
        "    end_len = len(\".png\")\n",
        "    fp_in = f\"{base_name}*.png\"\n",
        "    fp_out = f\"{base_name}.gif\"\n",
        "\n",
        "    img, *imgs = [Image.open(f) \n",
        "                  for f in sorted(glob.glob(fp_in), key=lambda x : int(x[base_len:-end_len]))]\n",
        "\n",
        "    img.save(fp=fp_out, format='GIF', append_images=imgs,\n",
        "             save_all=True, duration=1000, loop=0)\n",
        "    \n",
        "    return fp_out\n",
        "\n",
        "fn = make_gif(title)"
      ],
      "metadata": {
        "id": "YwrnxPddKkAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction and ground truth in png format\n",
        "from IPython.display import Image as Image_display\n",
        "with open(fn,'rb') as f:\n",
        "    display(Image_display(data=f.read(), format='png'))\n"
      ],
      "metadata": {
        "id": "kOjmff5MKq1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GgFJMZfXK7mc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Medical_Images_Segmentation_using_UNET.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMbcaSyj3Ks8VJ8H68aRlrv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}